{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import catboost as cb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA = 'test.csv'\n",
    "IS_TRAIN = False\n",
    "MODEL_SAVE_PATH = 'model.cb'\n",
    "RESULT_SAVE_PATH = 'result.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df = pd.read_csv(INPUT_DATA, sep='\\t', error_bad_lines=False, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_FEATURES = [\n",
    "    'chrom',\n",
    "    'inputPos',\n",
    "    'inputRef',\n",
    "    'inputAlt',\n",
    "    'transcript',\n",
    "    'codingEffect',\n",
    "    'varLocation',\n",
    "    'alt_pNomen',\n",
    "    'wtSSFScore',\n",
    "    'wtMaxEntScore',\n",
    "    'varSSFScore',\n",
    "    'varMaxEntScore',\n",
    "#     'rsId',\n",
    "#     'rsClinicalSignificance',\n",
    "    'rsMAF',\n",
    "    '1000g_AF',\n",
    "    'gnomadAltFreq_all',\n",
    "    'espAllMAF',\n",
    "    'espAllAAF',\n",
    "    'clinVarMethods',\n",
    "    'clinVarClinSignifs',\n",
    "    'nOrthos',\n",
    "    'conservedOrthos'\n",
    "] + [column for column in df.columns if 'score' in column] + ['MIM_disease']\n",
    "\n",
    "\n",
    "RULES = [\n",
    "    lambda df: df['codingEffect'] != 'synonymous',\n",
    "    lambda df: df['varLocation'] != 'intron',\n",
    "    lambda df: np.invert(df['1000g_AF'] > 0.01),\n",
    "    lambda df: np.invert(df['gnomadAltFreq_all'] > 0.01)\n",
    "]\n",
    "\n",
    "\n",
    "def rules_filter(df):\n",
    "    for rule in RULES:\n",
    "        df = df[rule(df)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_robust(df):\n",
    "    df = df[RELEVANT_FEATURES]  # Filtering relevant features\n",
    "    df = rules_filter(df)  # Rules filtering\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `inputPos` as regression parametr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_pos(df):\n",
    "    df = df.copy()\n",
    "    df.inputPos = df.inputPos.astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `clinVarMethods` split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVM_METHODS = {\n",
    "    'case-control': 0,\n",
    "    'clinical_testing': 1,\n",
    "    'curation': 2,\n",
    "    'in_vitro': 3,\n",
    "    'in_vivo': 4,\n",
    "    'literature_only': 5,\n",
    "    'not_provided': 6,\n",
    "    'phenotyping_only': 7,\n",
    "    'provider_interpretation': 8,\n",
    "    'reference_population': 9,\n",
    "    'research': 10,\n",
    "    'nan': 11\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_names(row, delimeters='|,'):\n",
    "    def unify(name):\n",
    "        return '_'.join(name.strip().lower().split())\n",
    "    \n",
    "    names = [row]\n",
    "    for delimeter in delimeters:\n",
    "        new_names = []\n",
    "        for name in names:\n",
    "            new_names.extend(name.split(delimeter))\n",
    "        names = new_names\n",
    "    return [unify(name) for name in names]\n",
    "\n",
    "\n",
    "def collect_names(column, delimeters='|,'): \n",
    "    all_names = set()\n",
    "    for row in column:\n",
    "        all_names.update(set(fetch_names(row)))\n",
    "    return all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_cvm(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df.clinVarMethods = df.clinVarMethods.fillna('nan')\n",
    "    cvm_features = []\n",
    "    for row in df.clinVarMethods:\n",
    "        row_features = np.zeros(len(CVM_METHODS))\n",
    "        for name in fetch_names(row):\n",
    "            row_features[CVM_METHODS[name]] += 1\n",
    "        cvm_features.append(row_features)\n",
    "    cvm_features = np.vstack(cvm_features)\n",
    "    \n",
    "    cvm_columns = ['cvm_' + m for _, m in sorted([(i, m) for m, i in CVM_METHODS.items()])]\n",
    "    cvm_features = pd.DataFrame(cvm_features, columns=cvm_columns, index=df.index)\n",
    "    df = pd.concat([df, cvm_features], axis=1)\n",
    "    del df['clinVarMethods']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRES = [\n",
    "    pre_robust,\n",
    "    pre_pos,\n",
    "    pre_cvm\n",
    "]\n",
    "\n",
    "\n",
    "def pre_all(df):\n",
    "    for pre in PRES:\n",
    "        df = pre(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pre_all(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCES = {\n",
    "    'benign': 0, \n",
    "    'likely_benign': 1, \n",
    "    'not_provided': 2, \n",
    "    'vus': 2, \n",
    "    'likely_pathogenic': 3, \n",
    "    'pathogenic': 4\n",
    "}\n",
    "\n",
    "\n",
    "def labelize_row(row):\n",
    "    return max((RELEVANCES[name] for name in fetch_names(row) if name in RELEVANCES), default=2)\n",
    "\n",
    "\n",
    "def labelize_target(y, bad_word='pathogenic'):\n",
    "    return y.apply(labelize_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_relevance(relevance):\n",
    "    return [r.lower() for it in relevance.split('|') for r in it.split(', ') if r.lower() in RELEVANCES]\n",
    "\n",
    "\n",
    "def get_relevances(significances):\n",
    "    return [split_relevance(significance) for significance in significances.values]\n",
    "\n",
    "\n",
    "def dcg(relevances):\n",
    "    return np.sum(2 ** relevances / np.log2(np.arange(2, relevances.size + 2)))\n",
    "\n",
    "\n",
    "def evaluate_serp(df, sign, score, n=30):\n",
    "    serp = df.sample(n=n, replace=False)\n",
    "    rel_true = np.array([RELEVANCES[np.random.choice(sign[index], size=1)[0]] for index, row in serp.iterrows()])\n",
    "\n",
    "    order_true = np.argsort(rel_true)[::-1]\n",
    "    serp = serp.iloc[order_true]\n",
    "    rel_true = rel_true[order_true]\n",
    "\n",
    "    order_pred = np.argsort(score[serp.index])[::-1]\n",
    "    rel_pred = rel_true[order_pred]\n",
    "    \n",
    "    return dcg(rel_pred) / dcg(rel_true)\n",
    "\n",
    "\n",
    "def evaluate(df, sign, score, k=1000, n=30):\n",
    "    np.random.seed(42)\n",
    "    return np.mean([evaluate_serp(df, sign, score, n) for _ in range(k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TT split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt_split(df, is_train):\n",
    "    np.random.seed(42)\n",
    "    df = df.copy()\n",
    "    \n",
    "    df.rename({'clinVarClinSignifs': 'y'}, axis=1, inplace=True)\n",
    "    del df['MIM_disease']\n",
    "    df = df.apply(lambda c: c.fillna('NaN') if (c.dtype == object) else c)\n",
    "    \n",
    "    if is_train:\n",
    "        df = df.loc[df.y.notna()]\n",
    "        sign = pd.Series(data=get_relevances(df.y), index=df.index)\n",
    "        not_empty = [it != [] for it in sign]\n",
    "        df = df[not_empty]\n",
    "        sign = sign[not_empty]\n",
    "        y = labelize_target(df.y)\n",
    "        \n",
    "        del df['y']\n",
    "        X = df\n",
    "        \n",
    "        X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        X_train, X_validate, y_train, y_validate = train_test_split(X_, y_, test_size=0.2)\n",
    "        data = (X_train, y_train), (X_validate, y_validate), (X_test, y_test)\n",
    "        cat_features = np.where(X.dtypes != np.float)[0]\n",
    "        return data, cat_features, sign\n",
    "    else:\n",
    "        del df['y']\n",
    "        X = df\n",
    "        cat_features = np.where(X.dtypes != np.float)[0]\n",
    "        return X.sample(frac=1), cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TRAIN:\n",
    "    data, cat_features, sign = tt_split(df, is_train=True)\n",
    "    (X_train, y_train), (X_validate, y_validate), (X_test, y_test) = data\n",
    "    print('TRAIN', X_train.shape, X_validate.shape, X_test.shape)\n",
    "else:\n",
    "    X, cat_features = tt_split(df, is_train=False)\n",
    "    print('TEST', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cb.CatBoostClassifier(iterations=300, loss_function='MultiClass')\n",
    "if IS_TRAIN:\n",
    "    model.fit(X_train, y_train, cat_features=cat_features, \n",
    "              use_best_model=True, eval_set=(X_validate, y_validate),\n",
    "              plot=False);\n",
    "    model.save_model(MODEL_SAVE_PATH)\n",
    "else:\n",
    "    model.load_model(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(model, df):\n",
    "    return model.predict(df)[:, 0] + model.predict_proba(df).max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TRAIN:\n",
    "    precision = (model.predict(X_test)[:, 0] == y_test).sum() / len(y_test)\n",
    "    score = pd.Series(data=calc_score(model, X_test), index=X_test.index)\n",
    "    ndcg = evaluate(X_test, sign, score)\n",
    "    print(f'precision={precision} ndcg={ndcg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_importance(columns, importances, top_k=20, save_path=None):\n",
    "    order = np.argsort(importances)[-top_k:]\n",
    "    \n",
    "    objects = columns[order]\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = np.array(importances)[order]\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.yticks(y_pos, objects)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature')\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TRAIN:\n",
    "    draw_importance(X_train.columns, model.feature_importances_, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TRAIN:\n",
    "    sns.distplot(y_test, kde=False);\n",
    "    sns.distplot(y_train, kde=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final flow\n",
    "\n",
    "1. Filter out all `y` $\\in [0, 1]$.\n",
    "2. Sort `codingEffect` (all but misence) according to score values.\n",
    "3. Sort all misence data (score) and append to back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_order(model, df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['y'] = model.predict(df)[:, 0].astype(int)\n",
    "    df = df[np.invert((df.y == 0) | (df.y == 1))]\n",
    "#     del df['y']\n",
    "    \n",
    "    df['score'] = -calc_score(model, df)\n",
    "    df['isCodingEffect'] = (df.codingEffect == 'missense').astype(int)\n",
    "    df = df.sort_values(by=['isCodingEffect', 'score'])\n",
    "#     del df['score']\n",
    "    df['score'] = -df['score'] - df['y']\n",
    "    del df['isCodingEffect']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test if IS_TRAIN else X\n",
    "of = final_order(model, X_test)\n",
    "X_test.shape, of.shape  # Plus `y` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(of.y, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(of.y == 4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of[of.y == 4].codingEffect.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[of[(of.y == 4) & (of.codingEffect != 'missense')].index].MIM_disease.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_TRAIN:\n",
    "    of_ = of.reset_index(drop=True)\n",
    "    of_[of_.y == 4].to_csv(RESULT_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
